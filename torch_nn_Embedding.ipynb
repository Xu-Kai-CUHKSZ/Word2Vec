{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMl9T+bz0eatoHfxUk2ri5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xu-Kai-CUHKSZ/Word2Vec/blob/main/torch_nn_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ```torch.nn.Embedding``` 模块介绍：理解词向量与实现\n",
        "\n",
        "## 1. 基础介绍\n",
        "```torch.nn.Embedding```的本质是一个映射表（Lookup table），它用于储存自然语言词典和嵌入向量的映射关系。\n",
        "\n",
        "### 1.1 基本参数\n",
        "\n",
        "```torch.nn.Embedding```的初始化接受两个基本参数：```num_embeddings``` 和```embedding_dim```。\n",
        "\n",
        "- num_embeddings：这个参数直观理解为“要嵌入的自然语言的词汇数量”，表示上面所述的自然语言词典的大小，即可能的唯一词汇数量。比如英语中的常用单词，从abandon开始一共有3000个，那num_embeddings就可以设定为3000；\n",
        "- embedding_dim：表示每个词汇映射的嵌入向量的维度。\n",
        "\n",
        "### 1.2 可选参数\n",
        "- ```padding_idx```：用于指定词汇表中的填充词汇索引，该位置的向量将被初始化为零。\n",
        "- ```max_norm```：用于限制嵌入向量的L2范数。\n",
        "- ```norm_type```：用于指定范数类型。\n",
        "- ```scale_grad_by_freq```：如果设置为```True```，则将梯度按词汇频率缩放。\n",
        "- ```sparse```：如果设置为```True```，则将嵌入梯度标记为稀疏。\n",
        "\n",
        "### 1.3 属性\n",
        "```torch.nn.Embedding``` 模块只有一个属性```weight```。这个属性代表了嵌入层要学习的权重，即存储所有嵌入向量的矩阵。这是嵌入层的学习权重，形状为(num_embeddings, embedding_dim)，也就是上文所说的lookup table映射表。这些权重代表实际的嵌入向量，它们是可学习的参数，并且在训练过程中会被优化算法更新。默认情况下，weight 是从标准正态分布 N(0, 1) 随机初始化的。这意味着每个元素都独立地从均值为 0、标准差为 1 的正态分布中采样。\n",
        "\n",
        "## 2. 实例演示\n",
        "这里我将给出一个简单的例子来说明如何使用 ```PyTorch``` 的 ```torch.nn.Embedding``` 模块创建一个嵌入层，并获取一些单词的嵌入向量。<p>\n",
        "\n",
        "假设我们有一个小型的词汇表，包含以下单词：\n",
        "\n",
        "- “the” ,“cat” ,“dog” ,“sat” ,“on” ,“mat”\n",
        "我们将这些单词映射到索引上，例如：\n",
        "\n",
        "- “the” -> 0\n",
        "- “cat” -> 1\n",
        "- “dog” -> 2\n",
        "- “sat” -> 3\n",
        "- “on” -> 4\n",
        "- “mat” -> 5\n",
        "<p>\n",
        "现在我们可以创建一个 ```torch.nn.Embedding``` 层，将这些单词映射到嵌入向量中。我们将使用一个 3 维的嵌入向量来表示每个单词。\n",
        "\n",
        "下面是具体的代码示例："
      ],
      "metadata": {
        "id": "FCPCYuam50Ue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mcW-r6e5nd_",
        "outputId": "f89c2c95-6062-473b-dd82-e65ca2b06734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.0929, -0.2383,  0.1697],\n",
            "        [-0.7809, -0.5097,  0.3627],\n",
            "        [ 1.1861,  0.9559,  0.0673],\n",
            "        [-0.2327,  1.1906,  0.7017],\n",
            "        [ 1.4833,  0.9622, -1.3184],\n",
            "        [-1.9579, -0.8621, -0.8344]], grad_fn=<EmbeddingBackward0>)\n",
            " \n",
            "tensor([[-1.0929, -0.2383,  0.1697],\n",
            "        [-0.7809, -0.5097,  0.3627],\n",
            "        [ 1.1861,  0.9559,  0.0673]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 创建一个 Embedding 层\n",
        "# num_embeddings: 词汇表的大小，这里是 6\n",
        "# embedding_dim: 嵌入向量的维度，这里是 3\n",
        "embedding = nn.Embedding(num_embeddings=6, embedding_dim=3)\n",
        "\n",
        "# 定义一些单词的索引\n",
        "word_indices = torch.tensor([0, 1, 2, 3, 4, 5])  # \"the\", \"cat\", \"dog\", \"sat\", \"on\", \"mat\"\n",
        "\n",
        "# 通过索引获取嵌入向量\n",
        "word_embeddings = embedding(word_indices)\n",
        "\n",
        "# 输出嵌入向量\n",
        "print(word_embeddings)\n",
        "print(' ')\n",
        "print(embedding(torch.tensor([0, 1, 2])))"
      ]
    }
  ]
}