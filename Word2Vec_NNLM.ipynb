{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBM+rBhEN4nyFPDLcgEPaJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xu-Kai-CUHKSZ/Word2Vec/blob/main/Word2Vec_NNLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbRbx8CJr8Td",
        "outputId": "716400e9-0b94-4d9a-9d80-f4cf5880f4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 神经网络语言模型（NNLM）\n",
        "- 本文参考[语言模型（二）—— 神经网络语言模型（NNLM）](https://blog.csdn.net/rongsenmeng2835/article/details/108571335) <p>\n",
        "\n",
        "**语言模型**：语言模型的作用就是检测一句话是不是正常人说出来的。语言模型形像化的解释是：给定一句话，看它是自然语言的概率P(w1,w2,w3,..,wt)是多少。<p>\n",
        "**词向量**：在神经网络语言模型中，词向量作为一个内部参数，跟神经网络中的其他内部参数一样都是先有一个随机初始化值，正向传播后计算损失函数再反向传播更新这些参数。这也就要求神经网络语言模型是有监督的学习，词向量是学习得到的副产物，也是模型内化的一部分。"
      ],
      "metadata": {
        "id": "rjTHbXLrnkEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![NNLM网络结构图](https://drive.google.com/uc?id=1VWXaZ9PQJsjD1g4Djhd6UDSj6Vrl9LKX)"
      ],
      "metadata": {
        "id": "c381kKFPEXB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 模型参数\n",
        "神经网络真正的输入： $ x = (C(w_{t-n+1}),C(w_{t-n+2}),...,C(w_{t-1})) $ <p>\n",
        "输出：$ y = y_{1} + y_{2} = b + Wx + Utanh(d+Hx)$\n",
        "- 这里 $y$的维度应该等于 $V$,即词汇表的大小。这样再将 $y$经过一个softmax函数做概率归一化，便能得到一个维度为$V$的概率向量，这就是我们的输出了。（找到最大的概率所在位置的索引，结合词表我们就能得到我们的预测值了）\n",
        "\n",
        "模型训练的目标是最大化以下似然函数：$L = \\frac{1}{T} \\sum_{t} log f(w_i, w_{i-1}, ..., w_{i-n+1}; \\theta) + R(\\theta)$ , 其中\n",
        "\n",
        "$$\\theta = (b, d, W, U, H, C).$$"
      ],
      "metadata": {
        "id": "XcxDnXCfGiPk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHe4BRKqnLb_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1.Basic Embedding Model\n",
        "    1-1. NNLM(Neural Network Language Model)\n",
        "\"\"\"\n",
        "\n",
        "dtype = torch.FloatTensor\n",
        "sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"]\n",
        "\n",
        "word_list = \" \".join(sentences).split()  # 制作词汇表， \" \".join(sentences)：将句子列表中的所有句子合并为一个单一的字符串\n",
        "print(word_list)\n",
        "word_list = list(set(word_list))  # 去除词汇表中的重复元素\n",
        "print(\"去重后的word_list:\", word_list)\n",
        "print(' ')\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}  # 将每个单词对应于相应的索引\n",
        "number_dict = {i: w for i, w in enumerate(word_list)}  # 将每个索引对应于相应的单词\n",
        "n_class = len(word_dict)  # 单词的总数\n",
        "print(f'n_class:{n_class}')\n",
        "print(' ')\n",
        "print(f'word_dict:{word_dict}')\n",
        "print(' ')\n",
        "print(f'numberr_dict:{number_dict}')\n",
        "\n",
        "# NNLM parameters\n",
        "n_step = 2   # 根据前两个单词预测第3个单词\n",
        "n_hidden = 2  # 隐藏层神经元的个数\n",
        "m = 2  # 词向量的维度"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShlEHeDyuc7N",
        "outputId": "3c872b3e-d8b1-4af9-b8a8-1b1e6578c4bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'like', 'dog', 'i', 'love', 'coffee', 'i', 'hate', 'milk']\n",
            "去重后的word_list: ['dog', 'like', 'love', 'coffee', 'milk', 'i', 'hate']\n",
            " \n",
            "n_class:7\n",
            " \n",
            "word_dict:{'dog': 0, 'like': 1, 'love': 2, 'coffee': 3, 'milk': 4, 'i': 5, 'hate': 6}\n",
            " \n",
            "numberr_dict:{0: 'dog', 1: 'like', 2: 'love', 3: 'coffee', 4: 'milk', 5: 'i', 6: 'hate'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 由于pytorch中输入的数据是以batch小批量进行输入的，下面的函数就是将原始数据以一个batch为基本单位喂给模型\n",
        "def make_batch(sentences):\n",
        "    \"\"\"\n",
        "    input:\n",
        "    sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\"]\n",
        "    output:\n",
        "    input_batch = [[1, 4], [1, 3], [1, 5]]\n",
        "    target_batch = [0, 6, 2]\n",
        "    \"\"\"\n",
        "    input_batch = []\n",
        "    target_batch = []\n",
        "    for sentence in sentences:\n",
        "        word = sentence.split()\n",
        "        input = [word_dict[w] for w in word[:-1]]   # 获取前n-1个单词\n",
        "        target = word_dict[word[-1]]           # 获取第n个单词\n",
        "        input_batch.append(input)\n",
        "        target_batch.append(target)\n",
        "    return input_batch, target_batch"
      ],
      "metadata": {
        "id": "6WDNlgY540O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_batch, target_batch = make_batch(sentences)\n",
        "print(f'input_batch: {input_batch}')\n",
        "print(f'target_batch: {target_batch}')\n",
        "input_batch = torch.LongTensor(input_batch)\n",
        "print(input_batch.size())\n",
        "print(input_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NONh92A6L5co",
        "outputId": "8551f976-b14b-45e9-877e-2649a8146a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_batch: [[5, 1], [5, 2], [5, 6]]\n",
            "target_batch: [0, 3, 4]\n",
            "torch.Size([3, 2])\n",
            "tensor([[5, 1],\n",
            "        [5, 2],\n",
            "        [5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "输出：$ y = y_{1} + y_{2} = b + Wx + Utanh(d+Hx)$"
      ],
      "metadata": {
        "id": "uX9brUPfzlWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class NNLM(nn.Module):    # 定义一个名为 NNLM 的类，它继承自 nn.Module。这是PyTorch中构建神经网络的基类\n",
        "    def __init__(self):\n",
        "        # super() 是一个内置函数，用于调用父类（这里是 nn.Module）的方法\n",
        "        # super(NNLM, self) 返回 NNLM 类的父类（nn.Module）的一个代理对象，通过这个代理，可以调用父类的方法。\n",
        "        # .__init__() 调用父类的初始化方法，确保父类的所有初始化步骤都被执行，特别是那些在 nn.Module 中定义的。\n",
        "        super(NNLM, self).__init__()\n",
        "        self.C = nn.Embedding(n_class, embedding_dim=m)  # 创建一个词嵌入层，用于将单词的索引映射到稠密的词向量, C这里表示词汇表矩阵\n",
        "        self.H = nn.Parameter(torch.randn(n_step * m, n_hidden).type(dtype))   # 隐藏层权重  H 为 4 x 2的矩阵\n",
        "        self.W = nn.Parameter(torch.randn(n_step * m, n_class).type(dtype))   # 定义矩阵W 4 x 7\n",
        "        self.d = nn.Parameter(torch.randn(n_hidden).type(dtype))          # 定义d:2\n",
        "        self.U = nn.Parameter(torch.randn(n_hidden, n_class).type(dtype))     # U: 2x7\n",
        "        self.b = nn.Parameter(torch.randn(n_class).type(dtype))          # b:7\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 输入x是一个包含单词索引的张量，经过词嵌入层后，x被转换为一个包含词向量的张量，形状为(batch_size=3, n_step = 2, m = 2)\n",
        "        # 其中 n_step是上下文窗口的大小，m是词向量的维度。\n",
        "        x = self.C(x)\n",
        "        x = x.view(-1, n_step * m) # x: [batch_size, n_step*n_class]\n",
        "\n",
        "        # torch.mm(x, self.H)：进行矩阵乘法，将输入 x（形状为 (batch_size, n_step * m)）与隐藏层权重 self.H（形状为 (n_step * m, n_hidden)）相乘\n",
        "        # 结果是tanh一个形状为 (batch_size, n_hidden) 的张量。\n",
        "        tanh = torch.tanh(self.d + torch.mm(x, self.H))\n",
        "\n",
        "\n",
        "        output = self.b + torch.mm(x, self.W) + torch.mm(tanh, self.U)\n",
        "        # output: [batch_size, n_class]\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "qDdjIeHw5How"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NNLM()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 制作输入\n",
        "input_batch, target_batch = make_batch(sentences)\n",
        "# 将输入和目标转换为 LongTensor\n",
        "input_batch = torch.LongTensor(input_batch)\n",
        "target_batch = torch.LongTensor(target_batch)\n",
        "\n",
        "# 开始训练\n",
        "for epoch in range(5000):\n",
        "    optimizer.zero_grad()    # 清零梯度，清除上一个周期计算的梯度。由于 PyTorch 默认会累积梯度，所以每个周期开始时需要将梯度清零。\n",
        "    output = model(input_batch)\n",
        "    # output : [batch_size=3, n_class=7], target_batch : [batch_size=3] (LongTensor, not one-hot)\n",
        "\n",
        "    loss = criterion(output, target_batch)   # 计算损失\n",
        "    if (epoch + 1) % 1000 == 0:    # 每 1000 个周期输出一次当前的周期数和损失值，以便监控训练过程\n",
        "        print(\"Epoch:{}\".format(epoch + 1), \"Loss:{:.3f}\".format(loss))\n",
        "    loss.backward()    # 执行反向传播，计算损失相对于模型参数的梯度\n",
        "    optimizer.step()   #更新模型的参数，根据计算得到的梯度调整参数，以减少损失\n",
        "\n",
        "# 预测\n",
        "# data.max(1, keepdim=True)：在每一行（每个样本）中找到最大值及其索引，返回一个元组，其中第二个元素是最大值的索引（即预测的类别）。\n",
        "# [1] 选择索引，得到预测类别的张量 predict，其形状为 [batch_size, 1]。\n",
        "predict = model(input_batch).data.max(1, keepdim=True)[1]  # [batch_size, n_class]\n",
        "print(\"predict: \\n\", predict)\n",
        "# 测试\n",
        "print([sentence.split()[:2] for sentence in sentences], \"---->\",\n",
        "      [number_dict[n.item()] for n in predict.squeeze()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GciGgidjK3Hw",
        "outputId": "9b1fa13b-e663-4236-d85d-6752907f3e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1000 Loss:0.109\n",
            "Epoch:2000 Loss:0.020\n",
            "Epoch:3000 Loss:0.007\n",
            "Epoch:4000 Loss:0.003\n",
            "Epoch:5000 Loss:0.001\n",
            "predict: \n",
            " tensor([[0],\n",
            "        [3],\n",
            "        [4]])\n",
            "[['i', 'like'], ['i', 'love'], ['i', 'hate']] ----> ['dog', 'coffee', 'milk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取训练后的词嵌入\n",
        "word_embeddings = model.C.weight.data\n",
        "print(word_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUYuSUTmYTfw",
        "outputId": "9ddef648-87c6-490a-e129-06d84dbb2bb5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.4772, -0.0403],\n",
            "        [-0.1820, -0.5141],\n",
            "        [-0.4413,  2.4713],\n",
            "        [-0.5664,  0.4791],\n",
            "        [-1.2249, -0.6533],\n",
            "        [ 0.5640,  1.1866],\n",
            "        [ 1.0607,  0.2699]])\n"
          ]
        }
      ]
    }
  ]
}